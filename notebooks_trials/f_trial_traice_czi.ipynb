{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the directory above the current notebook to the system path\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io_images import get_images_infoframe\n",
    "import paramiko"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get chunk images .tiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = \"nt\" in os.name\n",
    "folder_path = '/run/user/1001/gvfs/smb-share:server=fs.ista.ac.at,share=drives/tnegrell/archive/siegegrp/AlVe/MORPHOMICS2.0_MICROGLIA_BRAIN_ATLAS'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# windows = True\n",
    "# folder_path = r\"\\\\fs.ista.ac.at\\drives\\aventuri\\archive\\siegegrp\\AlVe\\MORPHOMICS2.0_MICROGLIA_BRAIN_ATLAS\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = os.path.join(folder_path, \"chunk_images\")\n",
    "\n",
    "infoframe = get_images_infoframe(project_path, \n",
    "                                 extension='.tif',\n",
    "                                 conditions = ['Age', 'Sex', 'Animal', 'Slide'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infoframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get one chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file name you are searching for\n",
    "search_file = \"microglia_Age_18m_Sex_F_Animal_1_Slide_0_seq_0_chunk_1_over_5_x__-127435_-122732__y__25685_35945__.tif\"\n",
    "\n",
    "# Get the index of the row containing the file name\n",
    "index = infoframe.loc[infoframe['file_name'] == search_file].index\n",
    "\n",
    "# If you want to print or use the index\n",
    "print(f\"Index of the row: {index}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_example = infoframe.iloc[263]\n",
    "\n",
    "age = chunk_example['Age']\n",
    "sex = chunk_example['Sex']\n",
    "animal = chunk_example['Animal']\n",
    "slide = chunk_example['Slide']\n",
    "filename = chunk_example['file_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1./0.324\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = f'/mnt/archive/archive/siegegrp/AlVe/MORPHOMICS2.0_MICROGLIA_BRAIN_ATLAS'\n",
    "function = f'CUDA_VISIBLE_DEVICES=0 trAIce img2swc'\n",
    "linux_folderpath = f' -ip {base_path}/chunk_images/{age}/{sex}/{animal}/{slide}/{filename}' \n",
    "\n",
    "cube_params = f' -wss \"(128, 128, 16)\" -wsb \"(128, 128, 16)\" -tp \"(3.08, 3.08, 1.0)\"'\n",
    "save_folderpath = f' -spd {base_path}/traced_microglia/{age}/{sex}/{animal}/{slide} -mp ./ -spsl ./ -nw 1 -bsp ./'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = function + linux_folderpath + cube_params + save_folderpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSH connection details\n",
    "ssh_user = \"tnegrell\"\n",
    "ssh_host = \"10.6.46.11\"\n",
    "remote_command = command\n",
    "password = \"123456\" \n",
    "\n",
    "# Create an SSH client\n",
    "ssh = paramiko.SSHClient()\n",
    "ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "\n",
    "try:\n",
    "    # Connect to the remote machine with a password\n",
    "    ssh.connect(ssh_host, username=ssh_user, password=password)\n",
    "\n",
    "    # Execute the command\n",
    "    stdin, stdout, stderr = ssh.exec_command(remote_command)\n",
    "\n",
    "    # Read and print the command's output\n",
    "    print(\"Command Output:\\n\", stdout.read().decode())\n",
    "    print(\"Command Error:\\n\", stderr.read().decode())\n",
    "finally:\n",
    "    # Close the SSH connection\n",
    "    ssh.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import click\n",
    "from . import preprocess as img_preprocess\n",
    "from . import model_io\n",
    "from . import postprocess\n",
    "import subprocess\n",
    "import ast\n",
    "from typing import Union\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@click.group(chain=True)\n",
    "def cli():\n",
    "    pass\n",
    "\n",
    "\n",
    "def img2soma_func(img_path, soma_model_save_path, soma_locs, window_size_soma,\n",
    "                  overlap, target_px2mu, save_path_dataset,\n",
    "                  mat_path, dataset_name, num_workers, ignore_flags) -> Union[bool, str]:\n",
    "    \"\"\" Detect soma from the image file and save the results to the save_path\n",
    "\n",
    "        Args:\n",
    "            img_path (str): The path to the image file\n",
    "            soma_model_save_path (str): The path to the soma model\n",
    "            soma_locs (str): The path to save the segmented somas of soma model\n",
    "            window_size_soma (tuple): The size of the window for soma detection cubes (soma model)\n",
    "            overlap (tuple): The overlap of the window for soma detection cubes (soma model)\n",
    "            target_px2mu (tuple): The target pixel to micron ratio used to scale the image\n",
    "            save_path_dataset (str): The path to save the cubes for soma detection\n",
    "            mat_path (str): The path to save the mat file\n",
    "            dataset_name (str): The name of the dataset\n",
    "            num_workers (int): The number of workers to use\n",
    "            soma_model_name (str): The name of the soma detection model in models class\n",
    "            ignore_flags (bool): Whether to ignore flags. If True, it will run the function regardless of previous computations\n",
    "        Returns:\n",
    "            str: The path to the saved soma locations\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert arguments str type to tuple\n",
    "    window_size_soma = str(window_size_soma)\n",
    "    overlap = str(overlap)\n",
    "    target_px2mu = str(target_px2mu)\n",
    "\n",
    "    window_size_soma = ast.literal_eval(window_size_soma)\n",
    "    overlap = ast.literal_eval(overlap)\n",
    "    target_px2mu = ast.literal_eval(target_px2mu)\n",
    "\n",
    "    # Set dataset name based on imaris file\n",
    "    if not dataset_name:\n",
    "        dataset_name = os.path.basename(img_path).split(\".\")[0]\n",
    "\n",
    "    # Set paths and flag paths\n",
    "    datasets_path = os.path.join(save_path_dataset, dataset_name)\n",
    "    if soma_locs is None:\n",
    "        soma_locs = os.path.join(datasets_path, 'soma')\n",
    "    else:\n",
    "        soma_locs = os.path.join(soma_locs, dataset_name)\n",
    "    flag_path_data = os.path.join(datasets_path, 'flag_data')\n",
    "    flag_path_cubes = os.path.join(datasets_path, 'flag_cubes')\n",
    "    flag_path_mat = os.path.join(mat_path, dataset_name, 'flag_mat')\n",
    "\n",
    "    # Check if flag is set. If so, terminate\n",
    "    if (not ignore_flags) and os.path.exists(flag_path_data):\n",
    "        print(\"IMG to Soma was done already.\")\n",
    "        return soma_locs\n",
    "\n",
    "    # Make mat file from img file\n",
    "    print(\"=======Make mat files out of img/nd2 file=======\")\n",
    "    print(f\"img_path: {img_path}\")\n",
    "    # Check mat flag\n",
    "    if ignore_flags or (not os.path.exists(flag_path_mat)):\n",
    "        # Run shell script to make mat file. main.sh handles file types: ims, nd2, tiff\n",
    "        # result = subprocess.run(\n",
    "        #     f\"./main.sh make_datasets_file \\\"{img_path}\\\" \\\"./ims_reader\\\" \\\"{mat_path}\\\"\", shell=True)\n",
    "            \n",
    "        # # Check if error in reading data\n",
    "        # if result.returncode != 0:\n",
    "        #     print(\"See error above. Error in reading data.\")\n",
    "        #     return False\n",
    "\n",
    "        # Check file type\n",
    "        if img_path.endswith('.ims'):\n",
    "            data = img_preprocess.make_dataset_ims(img_path, mat_path)\n",
    "        elif img_path.endswith('.nd2'):\n",
    "            data = img_preprocess.make_dataset_nd2(img_path, mat_path)\n",
    "        elif img_path.endswith('.tif'):\n",
    "            data = img_preprocess.make_dataset_tif(img_path, mat_path)\n",
    "\n",
    "        # Set flag for creating mat file\n",
    "        open(flag_path_mat, 'w').close()\n",
    "    else:\n",
    "        data = None\n",
    "        print(\"Mat file already exists.\")\n",
    "\n",
    "    print(f\"=======Make Masks Soma=======\")\n",
    "    # Extract window size, overlap, and target pixel to micrometer out of input arguments\n",
    "    window_size_x = window_size_soma[0]\n",
    "    window_size_y = window_size_soma[1]\n",
    "    window_size_z = window_size_soma[2]\n",
    "\n",
    "    overlap_x = overlap[0]\n",
    "    overlap_y = overlap[1]\n",
    "    overlap_z = overlap[2]\n",
    "\n",
    "    # target_px2mu is used to scale in a way to get this ratio\n",
    "    target_x_px2mu = target_px2mu[0]\n",
    "    target_y_px2mu = target_px2mu[1]\n",
    "    target_z_px2mu = target_px2mu[2]\n",
    "\n",
    "    print(\n",
    "        f\"Target pixel/micrometer: x={target_x_px2mu}, y={target_y_px2mu}, z={target_z_px2mu}\")\n",
    "\n",
    "    if not os.path.exists(save_path_dataset):\n",
    "        os.makedirs(save_path_dataset)\n",
    "\n",
    "    # Check cubes flag\n",
    "    if ignore_flags or (not os.path.exists(flag_path_cubes)):\n",
    "        img_preprocess.make_mask_soma(\n",
    "            data = data,\n",
    "            file_path=os.path.join(\n",
    "                mat_path,\n",
    "                dataset_name,\n",
    "                \"dataset.mat\"),\n",
    "            dataset_name=dataset_name,\n",
    "            save_path_dataset=save_path_dataset,\n",
    "            window_size_x=window_size_x,\n",
    "            window_size_y=window_size_y,\n",
    "            window_size_z=window_size_z,\n",
    "            overlap_x=overlap_x,\n",
    "            overlap_y=overlap_y,\n",
    "            overlap_z=overlap_z,\n",
    "            target_x_px2mu=target_x_px2mu,\n",
    "            target_y_px2mu=target_y_px2mu,\n",
    "            target_z_px2mu=target_z_px2mu,\n",
    "            num_workers=num_workers)\n",
    "        # Make file flag_path\n",
    "        open(flag_path_cubes, 'w').close()\n",
    "    else:\n",
    "        print(\"Cubes already exists.\")\n",
    "\n",
    "    print(f\"=======Soma Detection Stage=======\")\n",
    "    print(f\"Soma location: {os.path.join(soma_locs, dataset_name)}\")\n",
    "    model_io.soma_detection(\n",
    "        soma_model_save_path, datasets_path, num_workers, soma_locs)\n",
    "\n",
    "    # Make file flag_path\n",
    "    open(flag_path_data, 'w').close()\n",
    "\n",
    "    print(f\"Segmented soma saved in {soma_locs}\")\n",
    "\n",
    "    return soma_locs\n",
    "\n",
    "\n",
    "def soma2branch_func(branch_model_save_path, save_path, window_size_branch, save_path_soma_locs,\n",
    "                     kernel_size, branch_segmented_save_path, num_workers, postprocess_flag,\n",
    "                     ignore_flags, dataset_name) -> Union[bool, str]:\n",
    "    \"\"\" Detect branches after detecting somas\n",
    "\n",
    "        This function first extract soma locations from the output of soma detection stage,\n",
    "        and apply the branch detection model for each cell. So, it will produce segmented\n",
    "        branches for each cell in seperate folders and files.\n",
    "\n",
    "        Args:\n",
    "            branch_model_save_path (str): The path to the branch model\n",
    "            save_path (str): The path to the soma locations\n",
    "            window_size_branch (tuple): The size of the window for data of branch detection\n",
    "            save_path_soma_locs (str): The path to save the cubes for branch detection\n",
    "            kernel_size (int): The size of the kernel for adaptive histogram equalization\n",
    "            branch_segmented_save_path (str): The path to save the segmented branches\n",
    "            num_workers (int): The number of workers to use\n",
    "            postprocess_flag (bool): Whether to apply postprocessing. If True, it will apply dilation, select connected component of the detected soma, erosion, and the return the mask as branches of soma.\n",
    "            ignore_flags (bool): Whether to ignore flags. If True, it will run the function regardless of previous computations\n",
    "            dataset_name (str): The name of the dataset\n",
    "        Returns:\n",
    "            str: The path to the saved branch segmentation\n",
    "    \"\"\"\n",
    "    # Convert arguments str type to tuple\n",
    "    window_size_branch = str(window_size_branch)\n",
    "    window_size_branch = ast.literal_eval(window_size_branch)\n",
    "\n",
    "    # Set flag paths and save path\n",
    "    flag_path_branch = os.path.join(\n",
    "        branch_segmented_save_path, dataset_name, 'flag_branch')\n",
    "    flag_path_soma = os.path.join(\n",
    "        save_path_soma_locs, dataset_name, 'flag_soma')\n",
    "    save_path_soma_locs = os.path.join(\n",
    "        save_path_soma_locs, dataset_name, 'branches_data')\n",
    "    branch_segmented_save_path = os.path.join(\n",
    "        branch_segmented_save_path, dataset_name, 'branch')\n",
    "\n",
    "    # Check if the branch flag is set. If so, terminate\n",
    "    if (not ignore_flags) and os.path.exists(flag_path_branch):\n",
    "        print(\"IMG to Branch was done already.\")\n",
    "        return True\n",
    "\n",
    "    if not os.path.exists(save_path_soma_locs):\n",
    "        os.makedirs(save_path_soma_locs)\n",
    "\n",
    "    print(f\"=======Extract Soma Location=======\")\n",
    "    print(f\"------------loading data from: {save_path}\")\n",
    "    print(f\"------------saveing data to: {save_path_soma_locs}\")\n",
    "\n",
    "    # Extract window size for branch detection\n",
    "    window_size_x = window_size_branch[0]\n",
    "    window_size_y = window_size_branch[1]\n",
    "    window_size_z = window_size_branch[2]\n",
    "    print(\n",
    "        f\"window_size_x_branch: {window_size_x} window_size_y_branch: {window_size_y} window_size_z_branch: {window_size_z}\")\n",
    "    \n",
    "    # Check soma flag for creating cubes for branch detection\n",
    "    if ignore_flags or (not os.path.exists(flag_path_soma)):\n",
    "        postprocess.soma_loc_detection(save_path, save_path_soma_locs,\n",
    "                                       window_size_x, window_size_y, window_size_z,\n",
    "                                       kernel_size, num_workers)\n",
    "\n",
    "        # Make the flag for data of branch detection\n",
    "        open(flag_path_soma, 'w').close()\n",
    "    else:\n",
    "        print(\"Soma location was done already.\")\n",
    "\n",
    "    if not os.path.exists(branch_segmented_save_path):\n",
    "        os.makedirs(branch_segmented_save_path)\n",
    "\n",
    "    print(f\"======= Branch Segmentation =======\")\n",
    "    print(f\"Loading model from {branch_model_save_path}\")\n",
    "    model_io.branch_detection(branch_model_save_path, save_path_soma_locs,\n",
    "                              branch_segmented_save_path, postprocess_flag)\n",
    "\n",
    "    # Make the flag for branch detection\n",
    "    open(flag_path_branch, 'w').close()\n",
    "\n",
    "    postprocess.reconstruct_img_with_segmented_mask(dataset_name)target_x_px2mu\n",
    "    return True\n",
    "\n",
    "\n",
    "def branch2swc_func(branch_segmented_save_path) -> Union[bool, str]:\n",
    "    \"\"\" Extract SWC from the segmented branches\n",
    "\n",
    "        This function converts detected branches to SWC file from the output of soma2branch\n",
    "        stage. So, it will produce SWC files for each cell separately.\n",
    "\n",
    "        Args:\n",
    "            branch_segmented_save_path (str): The path to the segmented branches\n",
    "        Returns:\n",
    "            str: The path to save the SWC files\n",
    "    \"\"\"\n",
    "    # Extract SWC\n",
    "    postprocess.skeleton_extraction(branch_segmented_save_path)\n",
    "\n",
    "    print(f\"SWC saved in {branch_segmented_save_path}\")\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def cube2branch_func(cube, point, branch_model_save_path, posprocess_flag) -> Union[bool, str]:\n",
    "    \"\"\" Detect branches for one cube for GUI purpose\n",
    "\n",
    "        This function is used for GUI purpose to detect branches for one cube. It will\n",
    "        produce segmented branches for the cube.\n",
    "\n",
    "        Args:\n",
    "            cube (np.array): The cube for branch detection\n",
    "            point (tuple): The point for the cube\n",
    "            branch_model_save_path (str): The path to the branch model\n",
    "            posprocess_flag (bool): Whether to apply postprocessing. If True, it will apply dilation, select connected component of the detected soma, erosion, and the return the mask as branches of soma.\n",
    "        Returns:\n",
    "            str: The path to the saved branch segmentation\n",
    "    \"\"\"\n",
    "    cube = np.array(cube['arr_0'])\n",
    "    # remove char in point string\n",
    "    point = point.replace(\"'\", \"\")\n",
    "    point = ast.literal_eval(point)\n",
    "    print(type(point))\n",
    "    segmented = model_io.branch_detection_cube(cube, point, branch_model_save_path, posprocess_flag)\n",
    "    return segmented\n",
    "\n",
    "\n",
    "@cli.command('img2soma')\n",
    "@click.option('--img_path', '-ip', type=str, required=True)\n",
    "@click.option('--soma_model_save_path', '-smsp', type=str, default='/opt/trAIce/trAIce/models/models_best_\\soma_2024may07.pth')\n",
    "@click.option('--soma_locs', '-sl', type=str, required=False, default=None)\n",
    "@click.option('--window_size_soma', '-wss', type=str, required=True, default=\"(128,128,16)\")\n",
    "@click.option('--overlap', '-o', type=str, required=True, default=\"(0,0,0)\")\n",
    "@click.option('--target_px2mu', '-tp', type=str, required=True, default=\"(2.5,2.5,0.8)\")\n",
    "@click.option('--save_path_dataset', '-spd', type=str, required=True)\n",
    "@click.option('--mat_path', '-mp', type=str, required=True)\n",
    "@click.option('--dataset_name', '-dn', type=str, required=False, default=None)\n",
    "@click.option('--num_workers', '-nw', type=int, required=True, default=1)\n",
    "@click.option('--soma_model_name', '-smn', type=str, required=True, default=\"CellSomaSegmentationModel\")\n",
    "@click.option('--ignore_flags', '-if', type=bool, required=True, default=False)\n",
    "def img2soma(img_path, soma_model_save_path, soma_locs, window_size_soma,\n",
    "             overlap, target_px2mu, save_path_dataset,\n",
    "             mat_path, dataset_name, num_workers, soma_model_name, ignore_flags) -> bool:\n",
    "    \"\"\" This function uses appropriate function calling for conversion of each image to detected\n",
    "        somas.\n",
    "        Intermediate steps are making cubes for soma detection, feed the model, and save the\n",
    "        results.\n",
    "\n",
    "        Args:\n",
    "            img_path (str): The path to the image file\n",
    "            soma_model_save_path (str): The path to the soma model\n",
    "            soma_locs (str): The path to save the segmented somas of soma model, as default save in the dataset folder\n",
    "            window_size_soma (tuple): The size of the window for soma detection cubes (soma model), default is (128, 128, 16)\n",
    "            overlap (tuple): The overlap of the window for soma detection cubes (soma model), default is (0, 0, 0)\n",
    "            target_px2mu (tuple): The target pixel to micron ratio used to scale the image, default is (2.5, 2.5, 0.8)\n",
    "            save_path_dataset (str): The path to save the cubes for soma detection\n",
    "            mat_path (str): The path to save the mat file\n",
    "            dataset_name (str): The name of the dataset, as default is the name of the ims/nd2/tif image file\n",
    "            num_workers (int): The number of workers to use, default is 1\n",
    "            soma_model_name (str): The name of the soma detection model in models class\n",
    "            ignore_flags (bool): Whether to ignore flags. If True, it will run the function regardless of previous computations. default is False\n",
    "        Returns:\n",
    "            str: The path to the saved soma locations\n",
    "\n",
    "        Sample command:\n",
    "            python main.py img2soma --img_path \"test/CX3CR1-EGFP_RETINA_20X.ims\" --target_px2mu \"(2.5, 2.5, 0.8)\" --save_path_dataset \"tmp\" --mat_path \"tmp\" --num_workers 4\n",
    "            -smsp \"models/models_best_soma_2024may07.pth\"\n",
    "    \"\"\"\n",
    "\n",
    "    # Keep run time\n",
    "    start = time.time()\n",
    "\n",
    "    if not img2soma_func(img_path=img_path, soma_model_save_path=soma_model_save_path, soma_locs=soma_locs,\n",
    "                         window_size_soma=window_size_soma, overlap=overlap, target_px2mu=target_px2mu,\n",
    "                         save_path_dataset=save_path_dataset, mat_path=mat_path, dataset_name=dataset_name,\n",
    "                         num_workers=num_workers, ignore_flags=ignore_flags):\n",
    "        print(\"Error\")\n",
    "\n",
    "        return False\n",
    "\n",
    "    end = time.time()\n",
    "    print(\n",
    "        f\"====> Total Run Time: {int((end - start) / 3600)}h {int((end - start) % 3600 / 60)}m {int((end - start) % 3600 % 60)}s\")\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "@cli.command('img2branch')\n",
    "@click.option('--img_path', '-ip', type=str, required=True)\n",
    "@click.option('--soma_locs', '-sl', type=str, required=False, default=None)\n",
    "@click.option('--soma_model_save_path', '-smsp', type=str, default='/opt/trAIce/trAIce/models/models_best_\\soma_2024may07.pth')\n",
    "@click.option('--branch_model_save_path', '-bms', type=str, default='/opt/trAIce/trAIce/models/36_UNETPromptGiven_128_128_16/')\n",
    "@click.option('--window_size_soma', '-wss', type=str, required=True, default=\"(128, 128, 16)\")\n",
    "@click.option('--window_size_branch', '-wsb', type=str, required=True, default=\"(512, 512, 32)\")\n",
    "@click.option('--overlap', '-o', type=str, required=True, default=\"(0, 0, 0)\")\n",
    "@click.option('--target_px2mu', '-tp', type=str, required=True, default=(2.5, 2.5, 0.8))\n",
    "@click.option('--save_path_dataset', '-spd', type=str, required=True)\n",
    "@click.option('--mat_path', '-mp', type=str, required=True)\n",
    "@click.option('--save_path_soma_locs', '-spsl', type=str, required=True)\n",
    "@click.option('--dataset_name', '-dn', type=str, required=False, default=None)\n",
    "@click.option('--kernel_size', '-ks', type=int, required=True, default=3)\n",
    "@click.option('--branch_segmented_save_path', '-bsp', type=str, required=True)\n",
    "@click.option('--num_workers', '-nw', type=int, required=True)\n",
    "@click.option('--soma_model_name', '-smn', type=str, required=True, default=\"CellSomaSegmentationModel\")\n",
    "@click.option('--postprocess_flag', '-ppf', type=bool, required=True, default=False)\n",
    "@click.option('--ignore_flags', '-if', type=bool, required=True, default=False)\n",
    "def img2branch(img_path, soma_model_save_path, soma_locs, branch_model_save_path,\n",
    "               window_size_soma, window_size_branch, overlap, target_px2mu,\n",
    "               save_path_dataset, mat_path, dataset_name, save_path_soma_locs,\n",
    "               kernel_size, branch_segmented_save_path, num_workers, soma_model_name,\n",
    "               postprocess_flag, ignore_flags) -> bool:\n",
    "    \"\"\" This function uses appropriate function calling for conversion of each image to branches.\n",
    "        Intermediate steps are making cubes for soma detection, feed the model, and save the\n",
    "        results for soma detection, extract soma locations, feed the branch detection model,\n",
    "        save the results for branch detection.\n",
    "\n",
    "        Args:\n",
    "            img_path (str): The path to the image file\n",
    "            soma_model_save_path (str): The path to the soma model\n",
    "            soma_locs (str): The path to save the segmented somas of soma model, as default save in the dataset folder\n",
    "            window_size_soma (tuple): The size of the window for soma detection cubes (soma model), default is (128, 128, 16)\n",
    "            overlap (tuple): The overlap of the window for soma detection cubes (soma model), default is (0, 0, 0)\n",
    "            target_px2mu (tuple): The target pixel to micron ratio used to scale the image, default is (2.5, 2.5, 0.8)\n",
    "            save_path_dataset (str): The path to save the cubes for soma detection\n",
    "            mat_path (str): The path to save the mat file\n",
    "            dataset_name (str): The name of the dataset, as default is the name of the ims/nd2/tif image file\n",
    "            num_workers (int): The number of workers to use, default is 1\n",
    "            soma_model_name (str): The name of the soma detection model in models class\n",
    "            ignore_flags (bool): Whether to ignore flags. If True, it will run the function regardless of previous computations. default is False\n",
    "            branch_model_save_path (str): The path to the branch model\n",
    "            window_size_branch (tuple): The size of the cubes for data of branch detection, default is (512, 512, 32)\n",
    "            save_path_soma_locs (str): The path to save the cubes for branch detection\n",
    "            kernel_size (int): The size of the kernel for adaptive histogram equalization, default is 3\n",
    "        Returns:\n",
    "            str: The path to the saved branch segmentation\n",
    "    \"\"\"\n",
    "\n",
    "    # Keep run time\n",
    "    start = time.time()\n",
    "\n",
    "    if not img2soma_func(img_path=img_path, soma_model_save_path=soma_model_save_path, soma_locs=soma_locs,\n",
    "                         window_size_soma=window_size_soma, overlap=overlap, target_px2mu=target_px2mu,\n",
    "                         save_path_dataset=save_path_dataset, mat_path=mat_path, dataset_name=dataset_name,\n",
    "                         num_workers=num_workers, ignore_flags=ignore_flags):\n",
    "        print(\"Error\")\n",
    "        return False\n",
    "\n",
    "    if not soma2branch_func(branch_model_save_path=branch_model_save_path,\n",
    "                            save_path=[os.path.join(save_path_dataset,\n",
    "                                                    [os.path.basename(img_path).split(\".\")[0] if dataset_name is None else dataset_name][0],\n",
    "                                                    'soma') if soma_locs is None else soma_locs][0],\n",
    "                            window_size_branch=window_size_branch,\n",
    "                            save_path_soma_locs=save_path_soma_locs,\n",
    "                            dataset_name=os.path.basename(img_path).split(\".\")[0],\n",
    "                            kernel_size=kernel_size,\n",
    "                            branch_segmented_save_path=branch_segmented_save_path,\n",
    "                            num_workers=num_workers,\n",
    "                            postprocess_flag=postprocess_flag,\n",
    "                            ignore_flags=ignore_flags):\n",
    "        print(\"Error\")\n",
    "        return False\n",
    "\n",
    "    end = time.time()\n",
    "    print(\n",
    "        f\"====> Total Run Time: {int((end - start) / 3600)}h {int((end - start) % 3600 / 60)}m {int((end - start) % 3600 % 60)}s\")\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "@cli.command('img2swc')\n",
    "@click.option('--img_path', '-ip', type=str, required=True)\n",
    "@click.option('--soma_model_save_path', '-smsp', type=str, default='/opt/trAIce/trAIce/models/models_best_\\soma_2024may07.pth')\n",
    "@click.option('--branch_model_save_path', '-bms', type=str, default='/opt/trAIce/trAIce/models/36_UNETPromptGiven_128_128_16/')\n",
    "@click.option('--window_size_soma', '-wss', type=str, required=True, default=\"(128, 128, 16)\")\n",
    "@click.option('--window_size_branch', '-wsb', type=str, required=True, default=\"(128, 128, 16)\")\n",
    "@click.option('--overlap', '-o', type=str, required=True, default=\"(0, 0, 0)\")\n",
    "@click.option('--target_px2mu', '-tp', type=str, required=True, default=\"(2.5, 2.5, 0.8)\")\n",
    "@click.option('--save_path_dataset', '-spd', type=str, required=True)\n",
    "@click.option('--mat_path', '-mp', type=str, required=True)\n",
    "@click.option('--save_path_soma_locs', '-spsl', type=str, required=True)\n",
    "@click.option('--dataset_name', '-dn', type=str, required=False, default=None)\n",
    "@click.option('--soma_locs', '-sl', type=str, required=False, default=None)\n",
    "@click.option('--kernel_size', '-ks', type=int, required=True, default=3)\n",
    "@click.option('--branch_segmented_save_path', '-bsp', type=str, required=True)\n",
    "@click.option('--num_workers', '-nw', type=int, required=True)\n",
    "@click.option('--ignore_flags', '-if', type=bool, required=True, default=False)\n",
    "@click.option('--soma_model_name', '-smn', type=str, required=True, default=\"CellSomaSegmentationModel\")\n",
    "@click.option('--postprocess_flag', '-ppf', type=bool, required=True, default=False)\n",
    "@click.option('--ignore_flags', '-if', type=bool, required=True, default=False)\n",
    "def img2swc(img_path, soma_model_save_path, branch_model_save_path,\n",
    "            window_size_soma, window_size_branch, soma_locs,\n",
    "            overlap, target_px2mu, save_path_dataset,\n",
    "            mat_path, dataset_name, soma_model_name,\n",
    "            save_path_soma_locs, kernel_size, postprocess_flag,\n",
    "            branch_segmented_save_path, num_workers, ignore_flags) -> bool:\n",
    "    \"\"\" This function uses appropriate function calling for conversion of each image to SWC files.\n",
    "        Intermediate steps are making cubes for soma detection, feed the model, and save the\n",
    "        results for soma detection, extract soma locations, feed the branch detection model,\n",
    "        save the results for branch detection, and extract SWC files.\n",
    "\n",
    "        Args:\n",
    "            img_path (str): The path to the image file\n",
    "            soma_model_save_path (str): The path to the soma model\n",
    "            soma_locs (str): The path to save the segmented somas of soma model, as default save in the dataset folder\n",
    "            window_size_soma (tuple): The size of the window for soma detection cubes (soma model), default is (128, 128, 16)\n",
    "            overlap (tuple): The overlap of the window for soma detection cubes (soma model), default is (0, 0, 0)\n",
    "            target_px2mu (tuple): The target pixel to micron ratio used to scale the image, default is (2.5, 2.5, 0.8)\n",
    "            save_path_dataset (str): The path to save the cubes for soma detection\n",
    "            mat_path (str): The path to save the mat file\n",
    "            dataset_name (str): The name of the dataset, as default is the name of the ims/nd2/tif image file\n",
    "            num_workers (int): The number of workers to use, default is 1\n",
    "            soma_model_name (str): The name of the soma detection model in models class\n",
    "            ignore_flags (bool): Whether to ignore flags. If True, it will run the function regardless of previous computations. default is False\n",
    "            branch_model_save_path (str): The path to the branch model\n",
    "            window_size_branch (tuple): The size of the cubes for data of branch detection, default is (512, 512, 32)\n",
    "            save_path_soma_locs (str): The path to save the cubes for branch detection\n",
    "            kernel_size (int): The size of the kernel for adaptive histogram equalization, default is 3\n",
    "            postprocess_flag (bool): Whether to apply postprocessing. If True, it will apply dilation, select connected component of the detected soma, erosion, and the return the mask as branches of soma. default is False\n",
    "            branch_segmented_save_path (str): The path to save SWC files\n",
    "        Returns:\n",
    "            str: The path to the saved SWC files\n",
    "    \"\"\"\n",
    "\n",
    "    # Keep run time\n",
    "    start = time.time()\n",
    "\n",
    "    if not img2soma_func(img_path=img_path, soma_model_save_path=soma_model_save_path, soma_locs=soma_locs,\n",
    "                         window_size_soma=window_size_soma, overlap=overlap, target_px2mu=target_px2mu,\n",
    "                         save_path_dataset=save_path_dataset, mat_path=mat_path, dataset_name=dataset_name,\n",
    "                         num_workers=num_workers, ignore_flags=ignore_flags):\n",
    "        print(\"Error\")\n",
    "        return False\n",
    "\n",
    "    if not soma2branch_func(branch_model_save_path=branch_model_save_path,\n",
    "                            save_path=[os.path.join(save_path_dataset,\n",
    "                                                    [os.path.basename(img_path).split(\".\")[0] if dataset_name is None else dataset_name][0],\n",
    "                                                    'soma') if soma_locs is None else soma_locs][0],\n",
    "                            window_size_branch=window_size_branch,\n",
    "                            save_path_soma_locs=save_path_soma_locs,\n",
    "                            dataset_name=os.path.basename(img_path).split(\".\")[0],\n",
    "                            kernel_size=kernel_size,\n",
    "                            branch_segmented_save_path=branch_segmented_save_path,\n",
    "                            num_workers=num_workers,\n",
    "                            postprocess_flag=postprocess_flag,\n",
    "                            ignore_flags=ignore_flags):\n",
    "        print(\"Error\")\n",
    "        return False\n",
    "\n",
    "    # print(f\"branch_segmented_save_path {branch_segmented_save_path}\")\n",
    "    if not branch2swc_func(\n",
    "        branch_segmented_save_path=os.path.join(\n",
    "            branch_segmented_save_path, [\n",
    "            os.path.basename(img_path).split(\".\")[0] if dataset_name is None else dataset_name][0], 'branch')):\n",
    "        print(\"Error\")\n",
    "        return False\n",
    "\n",
    "    end = time.time()\n",
    "    print(\n",
    "        f\"====> Total Run Time: {int((end - start) / 3600)}h {int((end - start) % 3600 / 60)}m {int((end - start) % 3600 % 60)}s\")\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    cli()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mask_soma(data, file_path, dataset_name, save_path_dataset, target_x_px2mu, target_y_px2mu,\n",
    "                   target_z_px2mu, window_size_x, window_size_y, window_size_z, overlap_x,\n",
    "                   overlap_y, overlap_z, num_workers) -> Union[bool, str]:\n",
    "    \"\"\" Make cubes out of raw image with the configuration of soma detection model\n",
    "\n",
    "        This function is used in the main img2soma function.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): The path to the raw image file\n",
    "            dataset_name (str): The name of the dataset\n",
    "            save_path_dataset (str): The path to save the dataset\n",
    "            target_x_px2mu (float): The target pixel to micron ratio in x\n",
    "            target_y_px2mu (float): The target pixel to micron ratio in y\n",
    "            target_z_px2mu (float): The target pixel to micron ratio in z\n",
    "            window_size_x (int): The size of the cube in x\n",
    "            window_size_y (int): The size of the cube in y\n",
    "            window_size_z (int): The size of the cube in z\n",
    "            overlap_x (int): The overlap in x\n",
    "            overlap_y (int): The overlap in y\n",
    "            overlap_z (int): The overlap in z\n",
    "            num_workers (int): The number of workers to use\n",
    "        Returns:\n",
    "            str: The path to the saved dataset\n",
    "    \"\"\"\n",
    "    # Load mat file generated by Matlab\n",
    "    # dataset = mat73.loadmat(file_path)\n",
    "    if data is None:\n",
    "        try:\n",
    "            dataset = scipy.io.loadmat(file_path)\n",
    "        except Exception as e:\n",
    "            # load pickle file\n",
    "            with open(file_path, 'rb') as f:\n",
    "                dataset = pickle.load(f)\n",
    "        print(f\"keys in the mat file: {dataset.keys()}\")\n",
    "    else:\n",
    "        dataset = data\n",
    "    \n",
    "    img = dataset['img']\n",
    "\n",
    "    # Extract metadata sizing to create the cubes\n",
    "    ExtendMinX = np.squeeze(dataset['metadata']['ExtendMinX'])  # X starting point in microns\n",
    "    ExtendMinY = np.squeeze(dataset['metadata']['ExtendMinY'])  # Y starting point in microns\n",
    "    ExtendMinZ = np.squeeze(dataset['metadata']['ExtendMinZ'])  # Z starting point in microns\n",
    "    ExtendMaxX = np.squeeze(dataset['metadata']['ExtendMaxX'])  # X ending point in microns\n",
    "    ExtendMaxY = np.squeeze(dataset['metadata']['ExtendMaxY'])  # Y ending point in microns\n",
    "    ExtendMaxZ = np.squeeze(dataset['metadata']['ExtendMaxZ'])  # Z ending point in microns\n",
    "    SizeX = int(dataset['metadata']['SizeX'])  # X size in pixels\n",
    "    SizeY = int(dataset['metadata']['SizeY'])  # Y size in pixels\n",
    "    SizeZ = int(dataset['metadata']['SizeZ'])  # Z size in pixels\n",
    "    print(f\"metadata: {ExtendMinX} - {ExtendMinY} - {ExtendMinZ} - {ExtendMaxX} - {ExtendMaxY} - {ExtendMaxZ} - {SizeX} - {SizeY} - {SizeZ}\")\n",
    "\n",
    "    # X pixel to micron oroiginal ratio\n",
    "    origin_x_px2mu = SizeX / (ExtendMaxX - ExtendMinX)\n",
    "    # Y pixel to micron oroiginal ratio\n",
    "    origin_y_px2mu = SizeY / (ExtendMaxY - ExtendMinY)\n",
    "    # Z pixel to micron oroiginal ratio\n",
    "    origin_z_px2mu = SizeZ / (ExtendMaxZ - ExtendMinZ)\n",
    "    print(\n",
    "        f\"px2mu origin: {origin_x_px2mu} - {origin_y_px2mu} - {origin_z_px2mu}\")\n",
    "\n",
    "    # check if nan\n",
    "    print(origin_x_px2mu)\n",
    "    print(np.isnan(origin_x_px2mu))\n",
    "    if np.isnan(origin_x_px2mu):\n",
    "        size_x_new = SizeX\n",
    "        size_y_new = SizeY\n",
    "        size_z_new = SizeZ\n",
    "    else:\n",
    "        # X size in pixels after resizing based on pixel to micron ratio\n",
    "        size_x_new = int(SizeX / origin_x_px2mu * target_x_px2mu)\n",
    "        # Y size in pixels after resizing based on pixel to micron ratio\n",
    "        size_y_new = int(SizeY / origin_y_px2mu * target_y_px2mu)\n",
    "        # Z size in pixels after resizing based on pixel to micron ratio\n",
    "        size_z_new = int(SizeZ / origin_z_px2mu * target_z_px2mu)\n",
    "\n",
    "    print(f\"new size: {size_x_new} - {size_y_new} - {size_z_new}\")\n",
    "\n",
    "    ratio_x = size_x_new / SizeX  # X zoom factor\n",
    "    ratio_y = size_y_new / SizeY  # Y zoom factor\n",
    "    ratio_z = size_z_new / SizeZ  # Z zoom factor\n",
    "\n",
    "    # Scale image to have target pixel to micron ratio\n",
    "    print('- Scaling the image to target ...')\n",
    "    img = scipy.ndimage.zoom(img, (ratio_x, ratio_y, ratio_z), order=1)\n",
    "\n",
    "    # New image sizes\n",
    "    SizeX = img.shape[0]\n",
    "    SizeY = img.shape[1]\n",
    "    SizeZ = img.shape[2]\n",
    "\n",
    "    print(\n",
    "        f'- Making 3d images from somas and traces of the cells ...\\n Image size: {img.shape} - SizeX: {SizeX} - SizeY: {SizeY} - SizeZ: {SizeZ}')\n",
    "    if len(img.shape) > 3:\n",
    "        return print('Error: Image has more than 3 dimensions')\n",
    "\n",
    "    # Create the dataset img folder. img folder is used for saving the cubes for soma detection\n",
    "    save_path_dataset_img = os.path.join(\n",
    "        save_path_dataset, dataset_name, 'img')\n",
    "\n",
    "    # Create if the folder does not exist\n",
    "    if not os.path.exists(save_path_dataset_img):\n",
    "        os.makedirs(save_path_dataset_img)\n",
    "\n",
    "    print('- Slicing the image and masks into windows ...')\n",
    "\n",
    "    # Define a nested function as a core for multi-threading purpose\n",
    "    def make_mask_soma_core(input):\n",
    "        # Decode input\n",
    "        counter, i_counter, j_counter, k_counter = input\n",
    "\n",
    "        # Compute start and end points of the window\n",
    "        start_x = i_counter * (window_size_x - overlap_x)\n",
    "        start_y = j_counter * (window_size_y - overlap_y)\n",
    "        start_z = k_counter * (window_size_z - overlap_z)\n",
    "        end_x = start_x + window_size_x\n",
    "        end_y = start_y + window_size_y\n",
    "        end_z = start_z + window_size_z\n",
    "\n",
    "        # If the window is out of the image, adjust the window\n",
    "        if end_x > img.shape[0]:\n",
    "            end_x = img.shape[0]\n",
    "            start_x = end_x - window_size_x\n",
    "            if start_x < 0:\n",
    "                # The window is too big or image too small. Creating an error\n",
    "                print('Error: The window is too big or image too small')\n",
    "                return False\n",
    "\n",
    "        if end_y > img.shape[1]:\n",
    "            end_y = img.shape[1]\n",
    "            start_y = end_y - window_size_y\n",
    "            if start_y < 0:\n",
    "                # The window is too big or image too small. Creating an error\n",
    "                print('Error: The window is too big or image too small')\n",
    "                return False\n",
    "\n",
    "        if end_z > img.shape[2]:\n",
    "            end_z = img.shape[2]\n",
    "            start_z = end_z - window_size_z\n",
    "            if start_z < 0:\n",
    "                # The window is too big or image too small. Creating an error\n",
    "                print('Error: The window is too big or image too small')\n",
    "                return False\n",
    "\n",
    "        # Take out the cube out of image\n",
    "        window_img = img[start_x:end_x, start_y:end_y, start_z:end_z]\n",
    "\n",
    "        # Save the windows of image as .npz files\n",
    "        window_img = np.array(window_img, dtype=np.uint8)\n",
    "        np.savez(os.path.join(save_path_dataset_img,\n",
    "                 str(counter) + '.npz'), window_img)\n",
    "\n",
    "        return True\n",
    "\n",
    "    # Making a list for passing to threads. For multi-threading purpose\n",
    "    inputs = []\n",
    "    counter = 0\n",
    "    for i_counter in tqdm(range(0, int(img.shape[0] / (window_size_x - overlap_x)) + 1)):\n",
    "        for j_counter in range(0, int(img.shape[1] / (window_size_y - overlap_y)) + 1):\n",
    "            for k_counter in range(0, int(img.shape[2] // (window_size_z - overlap_z)) + 1):\n",
    "                inputs.append((counter, i_counter, j_counter, k_counter))\n",
    "                counter += 1\n",
    "\n",
    "    with futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        results = executor.map(make_mask_soma_core, inputs)\n",
    "        # check if there is any error\n",
    "        for result in results:\n",
    "            if not result:\n",
    "                return False\n",
    "\n",
    "    # Content of metadata saved as json in the dataset folder\n",
    "    metadata = {'window_size_x': window_size_x, 'window_size_y': window_size_y,\n",
    "                'window_size_z': window_size_z,\n",
    "                'img_size': img.shape,\n",
    "                'overlap_x': overlap_x, 'overlap_y': overlap_y, 'overlap_z': overlap_z,\n",
    "                'x_zoom_factor': ratio_x, 'y_zoom_factor': ratio_y, 'z_zoom_factor': ratio_z\n",
    "                }\n",
    "\n",
    "    with open(os.path.join(save_path_dataset, dataset_name, 'metadata.json'), 'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "\n",
    "    return os.path.join(save_path_dataset, dataset_name)\n",
    "\n",
    "def make_dataset_ims(file_path, mat_path):\n",
    "    file = ims(file_path)\n",
    "    dataset_name = os.path.basename(file_path).split('.')[0]\n",
    "\n",
    "    if file.Channels > 1 or file.TimePoints > 1:\n",
    "        print(\"Error: This is a 5D image\")\n",
    "        return 0\n",
    "\n",
    "    img = np.squeeze(file[:])\n",
    "    img = img.astype(np.float64)\n",
    "    img = img - np.min(img)\n",
    "    img = img / np.max(img)\n",
    "    img = np.uint8(img * 255)\n",
    "\n",
    "    dataset = {\n",
    "        'img': np.transpose(img, (2, 1, 0)),\n",
    "        'metadata': {\n",
    "            'ExtendMinX': 0,\n",
    "            'ExtendMinY': 0,\n",
    "            'ExtendMinZ': 0,\n",
    "            'ExtendMaxX': file.resolution[2]*img.shape[2],\n",
    "            'ExtendMaxY': file.resolution[1]*img.shape[1],\n",
    "            'ExtendMaxZ': file.resolution[0]*img.shape[0],\n",
    "            'SizeX': img.shape[2],\n",
    "            'SizeY': img.shape[1],\n",
    "            'SizeZ': img.shape[0],\n",
    "        }\n",
    "    }\n",
    "\n",
    "    path2matfile = os.path.join(mat_path, dataset_name, 'dataset' + '.mat')\n",
    "    data_io.save_dataset(path2matfile, dataset)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def make_dataset_nd2(file_path, mat_path):\n",
    "    file = nd2.ND2File(file_path)\n",
    "    dataset_name = os.path.basename(file_path).split('.')[0]\n",
    "\n",
    "    print(file.sizes)\n",
    "    if ('C' in file.sizes and file.sizes['C'] > 1) or ('T' in file.sizes and file.sizes['T'] > 1):\n",
    "        print(\"Error: This is a 5D image\")\n",
    "        exit(0)\n",
    "\n",
    "    metadata = {\n",
    "        'ExtendMinX': 0,\n",
    "        'ExtendMinY': 0,\n",
    "        'ExtendMinZ': 0,\n",
    "        'ExtendMaxX': file.voxel_size().x * file.sizes['X'],\n",
    "        'ExtendMaxY': file.voxel_size().y * file.sizes['Y'],\n",
    "        'ExtendMaxZ': file.voxel_size().z * file.sizes['Z'],\n",
    "        'SizeX': file.sizes['X'],\n",
    "        'SizeY': file.sizes['Y'],\n",
    "        'SizeZ': file.sizes['Z'],\n",
    "    }\n",
    "\n",
    "    img = nd2.imread(file_path)\n",
    "    img = np.transpose(img, (2, 1, 0))\n",
    "    img = img.astype(np.float64)\n",
    "    img = img - np.min(img)\n",
    "    img = img / np.max(img)\n",
    "    img = np.uint8(img * 255)\n",
    "\n",
    "    print(img.shape)\n",
    "    dataset = {\n",
    "        'img': img,\n",
    "        'metadata': metadata\n",
    "    }\n",
    "\n",
    "    path2matfile = os.path.join(mat_path, dataset_name, 'dataset' + '.mat')    \n",
    "    data_io.save_dataset(path2matfile, dataset)\n",
    "\n",
    "    file.close()\n",
    "\n",
    "    return dataset\n",
    "    \n",
    "def make_dataset_tif(file_path, mat_path):\n",
    "    print(file_path)\n",
    "    img = tiff.imread(file_path)\n",
    "    dataset_name = os.path.basename(file_path).split('.')[0]\n",
    "\n",
    "    img = np.array(img)\n",
    "    img = np.transpose(img, (2, 1, 0))\n",
    "    img = img.astype(np.float64)\n",
    "    img = img - np.min(img)\n",
    "    img = img / np.max(img)\n",
    "    img = np.uint8(img * 255)\n",
    "\n",
    "    print(img.shape)\n",
    "\n",
    "    metadata = {\n",
    "        'ExtendMinX': np.nan,\n",
    "        'ExtendMinY': np.nan,\n",
    "        'ExtendMinZ': np.nan,\n",
    "        'ExtendMaxX': np.nan,\n",
    "        'ExtendMaxY': np.nan,\n",
    "        'ExtendMaxZ': np.nan,\n",
    "        'SizeX': img.shape[0],\n",
    "        'SizeY': img.shape[1],\n",
    "        'SizeZ': img.shape[2],\n",
    "    }\n",
    "\n",
    "    dataset = {\n",
    "        'img': img,\n",
    "        'metadata': metadata\n",
    "    }\n",
    "\n",
    "    path2matfile = os.path.join(mat_path, dataset_name, 'dataset' + '.mat')\n",
    "    data_io.save_dataset(path2matfile, dataset)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioimag",
   "language": "python",
   "name": "bioimag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
